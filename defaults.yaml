name: "attention_rnn"
task_name: "svhn_digit"
shrink_dataset_by: 1
n_epochs: 100
batch_size: 100
hidden_dim: 256
n_patches: 8
patch_shape: [8, 8]
#patch_cnn_spec:
#    - border_mode: full
#      filter_size: [4, 4]
#      num_filters: 32
#    - border_mode: full
#      filter_size: [4, 4]
#      num_filters: 32
patch_mlp_spec: [128]
area_mlp_spec: [64]
# a final layer with dim=4*hidden_dim will be added to response_mlp_spec to comply with the LSTM interface
response_mlp_spec: []
learning_rate: 0.0001
batched_window: True
cutoff: 3
plot_url: http://bart2:5006/
location_std: 0.1
scale_std: 0.01
