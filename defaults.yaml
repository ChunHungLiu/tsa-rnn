name: "attention_rnn"
task_name: "svhn_digit"
patchmonitor_interval: 100
shrink_dataset_by: 1
patience_epochs: 100
max_epochs: 500
batch_size: 100
hidden_dim: 256
n_patches: 8
patch_shape: [8, 8]
#patch_cnn_spec:
#    - type: conv
#      border_mode: full
#      size: [4, 4]
#      num_filters: 32
#    - type: conv
#      border_mode: full
#      size: [4, 4]
#      num_filters: 32
#    - type: pool
#      size: [2, 2]
#      step: [2, 2]
patch_mlp_spec: [128]
locate_mlp_spec: [64]
merge_mlp_spec: [64]
response_mlp_spec: [128]
learning_rate: 0.0001
batched_window: True
cutoff: 3
location_std: 0.1
location_std_decay: 0.999
scale_std: 0.01
scale_std_decay: 0.999
classifier_dropout: 0.5
attention_dropout: 0.5
recurrent_dropout: 0.0
recurrent_weight_noise: 0.001
batch_normalize: False
batch_normalize_patch: True
plot_url: null
